\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\graphicspath{{Images/}{../Images/}}
\usepackage{subcaption} % for subfigure
\usepackage{stackengine} 
\usepackage{listings}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

%\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi

%%%%%%%%% TITLE
\title{Parallelizing Neural Networks with OpenMP and CUDA\\
\small{A Comparison Of Implementations and Optimizations}}

\author{Luis Guzman\\
University of Oregon\\
{\tt\small lguzmann@uoregon.edu}
\and
Steven Walton\\
University of Oregon\\
{\tt\small swalton2@uoregon.edu}
}

%\thispagestyle{empty}

\begin{document} 
%
\maketitle 
%
\section{Background} 
%
In the past decade, Neural Networks (NNs) have become increasingly popular, with advances in the area affecting almost every sector of computing. Deep Neural Networks (DNNs) \--- deeper, more complex models than their `shallow' counterparts \--- have progressed from being academic curiosities to being the cornerstone of many modern technologies.
Given this newfound relevance, many researchers have devoted a lot of time optimizing these algorithms, constantly trying to improve them for different accelerator technologies and platforms. Even long before accelerators, many efforts in trying to parallelize neural networks were made~\cite{10.1007/BFb0024235}.
While the new resurgence of machine learning (ML) partially came from the generalized availability of hardware accelerators like GPUs, there are still many innovations being made for CPUs. An example of this is Facebook's new Speech-to-Text (stt) system that runs on CPUs~\cite{fbcpu} and is aimed at home assistants that cannot rely on large and power-hungry accelerators.
As ML algorithms are getting larger, there are also continued efforts to apply machine learning techniques to simpler hardware. Thus, for the foreseeable future, optimizing ML algorithms on various platforms will remain an active area of research.

\section{Speeding Up Neural Architectures}
%
There are many areas that neural architectures can be sped up. Many different
calculations can be performed in parallel as they are not dependent on one
another. This can be from the feed forward section to the backwards propagation.
There are also parallel techniques for running the same model on different
machines. There are techniques for data parallelism like batch 
normalizing~\cite{DBLP:journals/corr/IoffeS15} that also improve performance,
which is arguably a speedup too (speedup to a certain convergence value). 

The most computationally intensive part of training a neural network is the backward pass, which is performed using the backwards propagation algorithm.
A neural network simply has a set of priors, data is fed in, an answer is given, and then the priors are updated based on how wrong the answer is from the true solution.
The way to update these priors is through backwards propagation.
Fundamentally, the main concept is performed by using a gradient descent technique, where the gradient is used to take a step in the direction towards a better solution.
Being the most computationally heavy section of the training process, it has drawn the most optimization efforts.
Many have used optimized libraries like cuBLAS to accelerate training~\cite{5708849} while others have developed new optimization algorithms~\cite{DBLP:journals/corr/KingmaB14}.

\section{Proposed Work}
%
Within this work we propose to test and evaluate different levels of parallelization within a neural network.
We will profile the algorithm to determine which sections of the code lend themselves most for optimization.
We will investigate the effects of different acceleration techniques, including OpenMP and CUDA.
For testing, we will perform performance and speed comparisons between our implementation and models coded using PyTorch~\cite{paszke2017automatic}, an optimized python library for creating neural networks. We will test against both PyTorch's CPU and GPU implementations.
To perform an accurate analysis we will compare how different parts of the network are parallelized or accelerated and thus determine which sections of the network are benefited the most from parallelization.
While we do not expect to match the optimization that PyTorch has, we can compare naive implementations as well as optimized libraries against professional software that is commonly used in industry.
By comparing both CPU, serial and parallel, and GPU implementations we can also gain a better understanding of how neural networks can be optimized for different architectures.
For example, GPUs are great at performing matrix multiplications while CPUs are more generalized compute units.

Much of this work will focus on comparing naive implementations vs optimal solutions, such as the Coppersmith-Winnograd Algorithm~\cite{COPPERSMITH1990251}. 
Matrix (and tensor) multiplication is the backbone of many parts of these neural networks, and comparing naive implementations to optimized algorithms and optimized software packages gives
insight into how the optimized packages work. 
By profiling the code we can also get a better insight into which areas need to be improved the most and we can quantify how much these types of algorithms affect neural networks. 

We believe that another key aspect in our project is to parallelize the gradient descent method.
There are existing efforts in parallelizing Stochastic Gradient Descent (SGD)~\cite{10.5555/2997046.2997185}, as well as other, more complex, gradient-based optimization algorithms, like  Adam~\cite{DBLP:journals/corr/KingmaB14}. 
One simple way to parallelize all of these algorithms is by performing batching, which we will explore in this work. 

Furthermore, we plan to do continued research, specifically into the optimizations and parallelized methods that PyTorch uses. 
At this point we do not know much about these algorithms but expect that expertise and interest to
grow as the project continues.

\section{Deliverables}
Our main objective is to produce source code that successfully implements a neural network with varying amounts of parallelization by utilizing different parallel programming platforms and techniques. 
In particular, we plan to implement the following versions:
\begin{itemize}
    \item Serial
    \item OpenMP
    \item CUDA
    \item Pytorch
    \item Pytorch (with GPU)
\end{itemize} 
%
We will also provide a visual comparison of our programs' performance to illustrate the differences of these distinct versions for different network sizes. 
%
Finally, we will provide our findings, conclusions, and ideas for future research in an in-depth report by the end on the term.

%
\bibliographystyle{abbrv}
%
\bibliography{references} 
%
\end{document}

