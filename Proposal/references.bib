@misc{fbcpu,
    title={A highly efficient, real-time text-to-speech system deployed on CPUs}, 
    url={https://ai.facebook.com/blog/a-highly-efficient-real-time-text-to-speech-system-deployed-on-cpus/}, 
    journal={ai.facebook.com}, 
    publisher={Facebook}, 
    author={He, Quin and Koehler, Thilo and D'Avirro, Antony and Gupta, Chetan}, 
    year={2020}, 
    month={May}}

@INPROCEEDINGS{5708849,
  author={X. {Sierra-Canto} and F. {Madera-Ramirez} and V. {Uc-Cetina}},
  booktitle={2010 Ninth International Conference on Machine Learning and Applications}, 
  title={Parallel Training of a Back-Propagation Neural Network Using CUDA}, 
  year={2010},
  volume={},
  number={},
  pages={307-312},}

@InProceedings{10.1007/BFb0024235,
author="Cristea, Alexandra I.
and Okamoto, Toshio",
editor="Polychronopoulos, Constantine
and Joe, Kazuki
and Araki, Keijiro
and Amamiya, Makoto",
title="A parallelization method for neural networks with weak connection design",
booktitle="High Performance Computing",
year="1997",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="397--404",
abstract="Hereby we present the construction and usage of ``Weak Connectiou''(WeCo) on Neural Networks(NN). We will show how these parallelization hypothesis increases the final system flexibility. The net design is based on standard procedures, but changed accordingly to WeCo parallelization principles. WeCo means parallelization with less weight on communication systems, as in: fine, medium and coarse grain parallelism, or between the parts of the implementation program. WeCo lays in-between parallel computers and sequential machines, building the bridge between them.",
isbn="978-3-540-69644-5"
}
  
@inproceedings{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/IoffeS15,
  author    = {Sergey Ioffe and
               Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing
               Internal Covariate Shift},
  journal   = {CoRR},
  volume    = {abs/1502.03167},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03167},
  archivePrefix = {arXiv},
  eprint    = {1502.03167},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/IoffeS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{COPPERSMITH1990251,
title = "Matrix multiplication via arithmetic progressions",
journal = "Journal of Symbolic Computation",
volume = "9",
number = "3",
pages = "251 - 280",
year = "1990",
note = "Computational algebraic complexity editorial",
issn = "0747-7171",
doi = "https://doi.org/10.1016/S0747-7171(08)80013-2",
url = "http://www.sciencedirect.com/science/article/pii/S0747717108800132",
author = "Don Coppersmith and Shmuel Winograd",
abstract = "We present a new method for accelerating matrix multiplication asymptotically. Thiswork builds on recent ideas of Volker Strassen, by using a basic trilinear form which is not a matrix product. We make novel use of the Salem-Spencer Theorem, which gives a fairly dense set of integers with no three-term arithmetic progression. Our resulting matrix exponent is 2.376."
}

@inproceedings{10.5555/2997046.2997185,
author = {Zinkevich, Martin A. and Weimer, Markus and Smola, Alex and Li, Lihong},
title = {Parallelized Stochastic Gradient Descent},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique — contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8].},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
pages = {2595–2603},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}
